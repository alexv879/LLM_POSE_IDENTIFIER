# Stage 1: Baseline Fine-Tuning with Sapiens-2B
# Configuration based on Meta Sapiens paper (ECCV 2024)

# Model Configuration
model:
  name: "sapiens_1b"
  backbone: "vit_base" # Vision Transformer Base (compromise for memory)
  pretrained: true
  pretrained_path: "facebook/sapiens-pretrain-1b-torchscript" # HuggingFace model (CORRECTED)
  num_keypoints: 17 # COCO format
  input_size: [384, 288] # [width, height] - REDUCED from 512x384 for 4GB GPU
  heatmap_size: [96, 72] # [width, height] - REDUCED (4x downsampling)
  sigma: 2.0 # Gaussian sigma for UDP heatmap generation
  use_udp: true # Enable UDP (Unbiased Data Processing) heatmap encoding

# Dataset Configuration
dataset:
  image_dir: "data/coco/images" # COCO images (train2017/val2017 subdirs)
  train_annotations: "data/coco/annotations/person_keypoints_train2017.json"
  val_annotations: "data/coco/annotations/person_keypoints_val2017.json"
  train_ratio: 0.8
  num_workers: 2 # REDUCED from 4 for 4GB GPU
  prefetch_factor: 2
  persistent_workers: true # ADDED: Keep workers alive between epochs
  drop_last: true # ADDED: Drop incomplete batches for stable training

# Training Configuration
training:
  # Two-phase training as per Sapiens paper (UPDATED to match official configs)
  # NOTE: Batch sizes reduced for 4GB VRAM (GTX 1650)
  phase1:
    name: "decoder_warmup"
    freeze_backbone: true # Only train decoder
    epochs: 10 # UPDATED from 3
    batch_size: 2 # REDUCED from 32 for 4GB GPU (use gradient accumulation)
    gradient_accumulation_steps: 16 # Accumulate to simulate batch_size=32
    learning_rate: 5.0e-4 # UPDATED from 1e-3
    optimizer: "adamw"
    weight_decay: 0.05 # UPDATED from 0.0001
    # Parameter-specific weight decay (official Sapiens config)
    param_groups:
      bias_decay_mult: 0.0
      norm_decay_mult: 0.0
      pos_embed_decay_mult: 0.0

  phase2:
    name: "full_finetuning"
    freeze_backbone: false # Train full model
    epochs: 100 # UPDATED from 20 (compromise between 210 and original)
    batch_size: 2 # REDUCED from 32 for 4GB GPU (use gradient accumulation)
    gradient_accumulation_steps: 16 # Accumulate to simulate batch_size=32
    learning_rate: 5.0e-4 # UPDATED from 2e-4
    optimizer: "adamw"
    weight_decay: 0.05 # UPDATED from 0.0001
    gradient_clip: 1.0
    # Parameter-specific weight decay
    param_groups:
      bias_decay_mult: 0.0
      norm_decay_mult: 0.0
      pos_embed_decay_mult: 0.0

  # Learning rate schedule (UPDATED to match official)
  scheduler:
    type: "cosine"
    warmup_epochs: 10 # UPDATED from 2 (10 epoch linear warmup)
    warmup_start_factor: 1.0e-4 # Start from very low LR
    min_lr: 5.0e-6 # UPDATED from 1e-6

  # Mixed precision training (FP16 for memory efficiency)
  mixed_precision: true

  # Loss function (UPDATED to match official)
  loss:
    type: "mse" # L2 heatmap loss (KeypointMSELoss)
    reduction: "mean"
    use_target_weight: true # UPDATED: Mask loss for invisible keypoints

# Data Augmentation (UPDATED to match official Sapiens pipeline)
augmentation:
  train:
    horizontal_flip: 0.5
    random_half_body: 0.3 # ADDED: RandomHalfBody augmentation
    rotation: 45 # degrees
    scale: [0.65, 1.35]
    shift: 0.1
    bbox_transform: # ADDED: RandomBBoxTransform
      scale_factor: [0.75, 1.5]
      rotation_factor: 60
    color_jitter:
      brightness: 0.4
      contrast: 0.4
      saturation: 0.4
      hue: 0.1
    gaussian_blur: 0.1
    # Albumentation transforms (official)
    albumentation:
      blur_prob: 0.1
      median_blur_prob: 0.1
      coarse_dropout: # ADDED: Random occlusion
        prob: 0.5
        max_holes: 1
        max_height: 0.4
        max_width: 0.4
        min_holes: 1
        min_height: 0.2
        min_width: 0.2

  val:
    # Flip test for validation (test-time augmentation)
    flip_test: true # ADDED
    resize_only: false # Changed

# Evaluation Configuration
evaluation:
  metrics: ["AP", "AP50", "AP75", "AR"]
  oks_sigmas: [
      0.026,
      0.025,
      0.025,
      0.035,
      0.035,
      0.079,
      0.079,
      0.072,
      0.072,
      0.062,
      0.062,
      0.107,
      0.107,
      0.087,
      0.087,
      0.089,
      0.089,
    ] # COCO standard sigmas
  save_predictions: true
  visualize_samples: 20

# Checkpointing
checkpoint:
  save_dir: "checkpoints/stage1"
  save_frequency: 5 # epochs
  keep_best_only: true
  monitor: "val_ap"
  mode: "max"

# Logging
logging:
  use_wandb: false # Set to true if using Weights & Biases
  use_tensorboard: true
  log_frequency: 50 # iterations
  project_name: "pose_llm_identifier"
  experiment_name: "stage1_baseline"

# Hardware
hardware:
  device: "cuda" # or "cpu"
  gpu_ids: [0]
  deterministic: true
  benchmark: false # Set true for consistent input sizes
  seed: 42
# Expected Performance (UPDATED based on fixes)
# Phase 1 (decoder only, 10 epochs): ~76-79% AP
# Phase 2 (full model, 100 epochs): ~80-84% AP
# With all fixes (UDP, augmentation, resolution): ~82-86% AP (matching paper baseline)
# Note: Original paper uses 210 epochs for 82-92% AP. Using 100 epochs as compromise.
