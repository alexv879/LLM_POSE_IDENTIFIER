# Stage 5: Post-Processing and LLM Integration
# Configuration for final refinement and interpretability

# Model Configuration
model:
  # Load VAE-refined predictions from Stage 4
  input_checkpoint: "checkpoints/stage4/best_model.pth"
  num_keypoints: 17

# Post-Processing Configuration
postprocessing:
  # OpenCV-based refinement
  opencv:
    enabled: true

    # Gaussian smoothing
    gaussian_blur:
      enabled: true
      kernel_size: 5 # Must be odd
      sigma: 1.0

    # Confidence thresholding
    confidence_threshold:
      enabled: true
      min_confidence: 0.3 # Filter out low-confidence keypoints
      adaptive: true # Use adaptive thresholding per joint

    # Boundary clipping
    boundary_clipping:
      enabled: true
      margin: 5 # pixels from image edge
      clip_to_person_bbox: true # Clip to detected person bbox if available

    # Temporal smoothing (for video sequences)
    temporal_smoothing:
      enabled: false # Set true for videos
      window_size: 5 # frames
      method: "moving_average" # moving_average, exponential, kalman

    # Anatomical constraints post-check
    anatomical_filtering:
      enabled: true
      max_limb_length: 500 # pixels (depends on image resolution)
      min_limb_length: 20 # pixels
      check_symmetry: true # Left/right limbs should be similar length
      symmetry_tolerance: 0.3 # 30% difference allowed

# LLM Integration Configuration
llm:
  enabled: true # Set to false if not using LLM
  provider: "openai" # Options: openai, anthropic, local

  # OpenAI Configuration
  openai:
    model: "gpt-4o" # or gpt-4o-mini, gpt-4-turbo
    api_key_env: "OPENAI_API_KEY" # Environment variable name
    temperature: 0.3
    max_tokens: 500

  # Anthropic Configuration (alternative)
  anthropic:
    model: "claude-3-5-sonnet-20241022"
    api_key_env: "ANTHROPIC_API_KEY"
    temperature: 0.3
    max_tokens: 500

  # Local LLM (alternative)
  local:
    model_path: "models/llama-3-8b-instruct"
    device: "cuda"
    max_tokens: 500

  # LLM Task Configuration
  tasks:
    # Task 1: Generate textual pose description
    pose_description:
      enabled: true
      prompt_template: |
        Given the following keypoint coordinates for a person:
        {keypoints_json}

        Generate a natural language description of the person's pose, including:
        1. Overall body position (standing, sitting, lying, etc.)
        2. Limb positions and orientations
        3. Notable gestures or movements
        4. Confidence and potential issues

        Keep the description concise (2-3 sentences).
      output_file: "pose_descriptions.json"

    # Task 2: Action recognition from pose
    action_recognition:
      enabled: true
      prompt_template: |
        Based on these keypoint positions:
        {keypoints_json}

        What action is this person most likely performing?
        Provide:
        1. Primary action (e.g., walking, running, sitting, waving)
        2. Confidence level (high/medium/low)
        3. Supporting evidence from keypoint positions

        Format as JSON: {{"action": "...", "confidence": "...", "reasoning": "..."}}
      output_file: "action_predictions.json"

    # Task 3: Quality assessment
    quality_assessment:
      enabled: true
      prompt_template: |
        Evaluate the quality of these pose predictions:
        {keypoints_json}

        Assess:
        1. Completeness (all keypoints present?)
        2. Anatomical plausibility (realistic joint angles?)
        3. Confidence distribution
        4. Potential issues or occlusions

        Provide a quality score (0-100) and brief explanation.
      output_file: "quality_assessments.json"

    # Task 4: Error correction suggestions
    error_correction:
      enabled: false # Optional advanced feature
      prompt_template: |
        Given these keypoint predictions:
        {keypoints_json}

        Identify any potential errors or anatomically impossible configurations.
        Suggest corrections if needed.

# Dataset Configuration
dataset:
  image_dir: "data/raw"
  test_annotations: "data/annotations/test_keypoints.json"
  num_workers: 4

  # Optional: Video sequences
  video_mode: false
  video_dir: "data/videos"
  fps: 30

# Evaluation Configuration
evaluation:
  metrics: ["AP", "AP50", "AP75", "AR"]
  oks_sigmas:
    [
      0.026,
      0.025,
      0.025,
      0.035,
      0.035,
      0.079,
      0.079,
      0.072,
      0.072,
      0.062,
      0.062,
      0.107,
      0.107,
      0.087,
      0.087,
      0.089,
      0.089,
    ]
  save_predictions: true
  visualize_samples: 50

  # Before/after post-processing comparison
  compare_with_stage4: true

  # LLM evaluation
  evaluate_llm_outputs: true
  human_evaluation: false # Set true for manual quality checks

# Output Configuration
output:
  save_dir: "outputs/stage5"

  # Save formats
  save_json: true
  save_coco_format: true
  save_visualizations: true
  save_llm_descriptions: true

  # Video output (if video_mode enabled)
  save_annotated_video: false
  video_codec: "mp4v"
  video_fps: 30

# Checkpointing (for any learned components)
checkpoint:
  save_dir: "checkpoints/stage5"
  monitor: "val_ap"
  mode: "max"

# Logging
logging:
  use_wandb: false
  use_tensorboard: true
  log_frequency: 25
  project_name: "pose_llm_identifier"
  experiment_name: "stage5_postprocess_llm"

  # Log LLM interactions
  log_llm_prompts: true
  log_llm_responses: true
  log_api_costs: true # Track API usage costs

# Hardware
hardware:
  device: "cuda"
  gpu_ids: [0]
  deterministic: true
  benchmark: false
  seed: 42

# Performance Optimization
optimization:
  batch_llm_requests: true # Batch multiple requests to save API calls
  cache_llm_responses: true # Cache responses for identical inputs
  parallel_processing: true # Process multiple images in parallel
  num_workers: 4
# Expected Performance
# Stage 4 (VAE): 94-97% AP
# Stage 5 (Post-processing): 95-98% AP (+1% improvement)
# - OpenCV refinement reduces noise
# - Confidence filtering removes false positives
# - LLM adds interpretability and explainability
# - Final system achieves near state-of-the-art performance
