# Quick Start Guide
## Pose LLM Identifier - Complete Pipeline

This guide will help you quickly get started with the complete 5-stage pose estimation pipeline.

---

## ðŸ“‹ Prerequisites

### System Requirements
- **GPU**: NVIDIA GPU with 8GB+ VRAM (RTX 3060/4060 or better)
- **RAM**: 16GB+ system memory
- **Storage**: 50GB+ free space
- **OS**: Windows 10/11, Linux, or macOS

### Software Requirements
- Python 3.8+
- CUDA 11.8+ (for GPU support)
- Git

---

## ðŸš€ Installation (5 Minutes)

### Step 1: Clone Repository
```powershell
cd "d:\Research Paper Pose LLM Identifier"
cd pose_llm_identifier
```

### Step 2: Create Virtual Environment
```powershell
# Create virtual environment
python -m venv venv

# Activate (Windows PowerShell)
.\venv\Scripts\Activate.ps1

# Or activate (Windows CMD)
venv\Scripts\activate.bat
```

### Step 3: Install Dependencies
```powershell
# Upgrade pip
python -m pip install --upgrade pip

# Install PyTorch with CUDA support
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Install other requirements
pip install -r requirements.txt
```

---

## ðŸ“ Data Preparation

### Option 1: Use Sample COCO Data
```powershell
# Create data directories
mkdir -p data\raw data\annotations

# Download COCO sample (replace with your data)
# Place images in: data\raw\
# Place annotations in: data\annotations\train_keypoints.json
#                       data\annotations\val_keypoints.json
```

### Option 2: Use Your Own Data

Your dataset must be in **COCO keypoint format**:

```json
{
  "images": [
    {
      "id": 1,
      "file_name": "image1.jpg",
      "width": 640,
      "height": 480
    }
  ],
  "annotations": [
    {
      "id": 1,
      "image_id": 1,
      "category_id": 1,
      "keypoints": [x1, y1, v1, x2, y2, v2, ..., x17, y17, v17],
      "bbox": [x, y, width, height]
    }
  ],
  "categories": [
    {
      "id": 1,
      "name": "person",
      "keypoints": ["nose", "left_eye", ...],
      "skeleton": [[16, 14], [14, 12], ...]
    }
  ]
}
```

**Keypoint Order (17 COCO keypoints):**
1. nose, 2. left_eye, 3. right_eye, 4. left_ear, 5. right_ear
6. left_shoulder, 7. right_shoulder, 8. left_elbow, 9. right_elbow
10. left_wrist, 11. right_wrist, 12. left_hip, 13. right_hip
14. left_knee, 15. right_knee, 16. left_ankle, 17. right_ankle

**Visibility values:** 0 = not labeled, 1 = labeled but not visible, 2 = labeled and visible

---

## âš™ï¸ Configuration

All configurations are in `configs/` directory. Update data paths:

```yaml
# configs/stage1_config.yaml
dataset:
  image_dir: "data/raw"
  train_annotations: "data/annotations/train_keypoints.json"
  val_annotations: "data/annotations/val_keypoints.json"
```

---

## ðŸƒ Running the Pipeline

### Option 1: Run All Stages (Recommended)
```powershell
# Run complete pipeline (Stage 1 â†’ 5)
python run_pipeline.py --all
```

This will:
- Train Stage 1 baseline (Sapiens-2B) â†’ **82-85% AP**
- Train Stage 2 SSL â†’ **89-93% AP**
- Train Stage 3 ensemble â†’ **92-95% AP**
- Train Stage 4 VAE â†’ **94-97% AP**
- Run Stage 5 post-processing â†’ **95-98% AP**

### Option 2: Run Individual Stages
```powershell
# Stage 1: Baseline fine-tuning
python run_pipeline.py --stage 1

# Stage 2: Semi-supervised learning
python run_pipeline.py --stage 2

# Stage 3: Ensemble fusion
python run_pipeline.py --stage 3

# Stage 4: VAE refinement
python run_pipeline.py --stage 4

# Stage 5: Post-processing + LLM
python run_pipeline.py --stage 5
```

### Option 3: Run Range of Stages
```powershell
# Run stages 2-4
python run_pipeline.py --start 2 --end 4
```

---

## ðŸ“Š Monitoring Training

### TensorBoard
```powershell
# In a separate terminal
tensorboard --logdir=runs/

# Open browser to: http://localhost:6006
```

### Check Outputs
```
checkpoints/
â”œâ”€â”€ stage1/
â”‚   â”œâ”€â”€ stage1_best.pth      # Best model from Stage 1
â”‚   â””â”€â”€ stage1_latest.pth    # Latest checkpoint
â”œâ”€â”€ stage2/
â”‚   â”œâ”€â”€ stage2_best.pth
â”‚   â””â”€â”€ training_curves.png  # Visualizations
â”œâ”€â”€ stage3/
â”œâ”€â”€ stage4/
â””â”€â”€ stage5/

outputs/
â”œâ”€â”€ validation/              # Validation visualizations
â”œâ”€â”€ predictions/            # JSON predictions
â””â”€â”€ stage5/                # Final results
```

---

## ðŸŽ¯ Quick Test (No Training)

If you want to test the code without full training:

```powershell
# Validate your annotations
python scripts/validate_annotations.py \
    --annotation_file data/annotations/train_keypoints.json \
    --image_dir data/raw \
    --output_dir outputs/validation

# This will generate:
# - Validation report
# - Sample annotated images
# - Dataset statistics
```

---

## ðŸ”§ Troubleshooting

### CUDA Out of Memory
Reduce batch size in config files:
```yaml
training:
  batch_size: 4  # Reduce from 8
```

### Missing Pretrained Models
For Stage 3, download pretrained models:
```powershell
# Create pretrained directory
mkdir pretrained\dwpose pretrained\vitpose

# Download (or use placeholder)
# DWPose: https://github.com/IDEA-Research/DWPose
# ViTPose: https://github.com/ViTAE-Transformer/ViTPose
```

### LLM Integration (Stage 5)
Set up API keys:
```powershell
# For OpenAI
$env:OPENAI_API_KEY="your-api-key-here"

# For Anthropic Claude
$env:ANTHROPIC_API_KEY="your-api-key-here"
```

Or disable LLM in config:
```yaml
# configs/stage5_config.yaml
llm:
  enabled: false
```

---

## ðŸ“ˆ Expected Performance Progression

| Stage | Method | Expected AP | Improvement |
|-------|--------|------------|-------------|
| **Stage 1** | Sapiens-2B Baseline | 82-85% | - |
| **Stage 2** | + SSL Multi-Path | 89-93% | +6-8% |
| **Stage 3** | + Ensemble Fusion | 92-95% | +3-2% |
| **Stage 4** | + VAE Refinement | 94-97% | +2% |
| **Stage 5** | + Post-Processing | **95-98%** | +1% |

---

## ðŸŽ“ Understanding Each Stage

### Stage 1: Foundation Model
- Uses Meta's Sapiens-2B (2B parameters)
- Pretrained on 300M human images
- Two-phase training: decoder warmup â†’ full finetuning

### Stage 2: Semi-Supervised Learning
- Leverages unlabeled data (5000 COCO images)
- 3 synergistic augmentation paths:
  - Geometry (rotation, perspective)
  - Appearance (color, blur, noise)
  - Occlusion (cutout, dropout)

### Stage 3: Ensemble
- Combines 3 models: Sapiens-2B, DWPose, ViTPose
- Confidence-weighted fusion
- Test-time augmentation (8 variants)
- Iterative refinement with SE attention

### Stage 4: Anatomical Plausibility
- VAE learns pose distribution
- Filters impossible poses
- Checks bone lengths and symmetry

### Stage 5: Final Polish
- OpenCV refinement (smoothing, clipping)
- LLM integration for:
  - Natural language pose descriptions
  - Action recognition
  - Quality assessment

---

## ðŸ“ž Getting Help

**Check logs:**
```powershell
# Logs are printed to console
# Save to file:
python run_pipeline.py --all > pipeline.log 2>&1
```

**Common issues:**
1. **ModuleNotFoundError**: Install missing package with `pip install <package>`
2. **CUDA errors**: Update GPU drivers or use CPU mode
3. **FileNotFoundError**: Check data paths in config files

---

## âœ… Verification Checklist

Before running the full pipeline:

- [ ] Virtual environment activated
- [ ] All dependencies installed (`pip list`)
- [ ] Data in correct format (COCO keypoints)
- [ ] Config files updated with correct paths
- [ ] GPU available (`python -c "import torch; print(torch.cuda.is_available())"`)
- [ ] Sufficient disk space (50GB+)
- [ ] (Optional) API keys set for LLM

---

## ðŸš€ Next Steps

After successful completion:

1. **Evaluate on test set:**
   ```powershell
   python stages/stage5_postprocess.py --config configs/stage5_config.yaml
   ```

2. **Export for deployment:**
   - Models saved in `checkpoints/`
   - Predictions in `outputs/`

3. **Integrate into your application:**
   - Load Stage 5 model
   - Run inference on new images
   - Use LLM descriptions for UX

---

## ðŸ“š Additional Resources

- **COCO Dataset**: https://cocodataset.org/
- **Sapiens Paper**: Meta ECCV 2024
- **ViTPose Paper**: NeurIPS 2022
- **DWPose Paper**: ICCV 2023

---

**Ready to start?** Run:
```powershell
python run_pipeline.py --all
```

ðŸŽ‰ **Good luck with your pose estimation project!** ðŸŽ‰
